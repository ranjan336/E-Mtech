{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ranjan.patra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ranjan.patra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (227371831.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [109]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(df.[0])\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = r\"C:\\Users\\ranjan.patra\\OneDrive - Lingaro Sp. z o. o\\DATA\\IITJ\\Course\\3rd Sem\\NLP\\Assignment_2\\data.csv\"  # Replace 'path_to_your_file.csv' with the actual path to your CSV file\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.Sentence[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5842\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>3130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment  count\n",
       "0  negative    860\n",
       "1   neutral   3130\n",
       "2  positive   1852"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(df))\n",
    "\n",
    "display(df.groupby('Sentiment').size().reset_index(name='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SentenceID Sentiment                                  Original Sentence  \\\n",
      "0            S0  positive  The GeoSolutions technology will leverage Bene...   \n",
      "1            S1  negative  $ESI on lows, down $1.50 to $2.50 BK a real po...   \n",
      "2            S2  positive  For the last quarter of 2010 , Componenta 's n...   \n",
      "3            S3   neutral  According to the Finnish-Russian Chamber of Co...   \n",
      "4            S4   neutral  The Swedish buyout firm has sold its remaining...   \n",
      "...         ...       ...                                                ...   \n",
      "5837      S5837  negative  RISING costs have forced packaging producer Hu...   \n",
      "5838      S5838   neutral  Nordic Walking was first used as a summer trai...   \n",
      "5839      S5839   neutral  According shipping company Viking Line , the E...   \n",
      "5840      S5840   neutral  In the building and home improvement trade , s...   \n",
      "5841      S5841  positive  HELSINKI AFX - KCI Konecranes said it has won ...   \n",
      "\n",
      "                                         Processed_text  \n",
      "0     [geosolutions, technology, leverage, benefon, ...  \n",
      "1          [esi, lows, 150, 250, bk, real, possibility]  \n",
      "2     [last, quarter, 2010, componenta, net, sales, ...  \n",
      "3     [according, finnishrussian, chamber, commerce,...  \n",
      "4     [swedish, buyout, firm, sold, remaining, 224, ...  \n",
      "...                                                 ...  \n",
      "5837  [rising, costs, forced, packaging, producer, h...  \n",
      "5838  [nordic, walking, first, used, summer, trainin...  \n",
      "5839  [according, shipping, company, viking, line, e...  \n",
      "5840  [building, home, improvement, trade, sales, de...  \n",
      "5841  [helsinki, afx, kci, konecranes, said, order, ...  \n",
      "\n",
      "[5842 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "preprocessed_data = []\n",
    "for i in range(len(df)):\n",
    "    # print(i)\n",
    "    # print(df.Sentence[i])\n",
    "    #text = \"This is an example sentence for preprocessing.\"\n",
    "    preprocessed_text = preprocess_text(df['Sentence'][i])\n",
    "    preprocessed_data.append({\n",
    "    'SentenceID':\"S\"+str(i),\n",
    "    'Sentiment':df.Sentiment[i],\n",
    "    'Original Sentence':df.Sentence[i],\n",
    "    'Processed_text':preprocessed_text}\n",
    "                              )\n",
    "\n",
    "preprocessed_df = pd.DataFrame(preprocessed_data)\n",
    "print(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceID                                                        S100\n",
      "Sentiment                                                     positive\n",
      "Original Sentence    Hargreaves Lansdown bucks weak markets to see ...\n",
      "Processed_text       [hargreaves, lansdown, bucks, weak, markets, s...\n",
      "Name: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_df.iloc[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example preprocessed data (list of tokenized sentences)\n",
    "preprocessed_data = [\n",
    "    ['this', 'is', 'an', 'example', 'sentence'],\n",
    "    ['another', 'example', 'sentence'],\n",
    "    ['yet', 'another', 'example', 'sentence'],\n",
    "    ['one', 'more', 'example', 'sentence'],\n",
    "    ['and', 'one', 'more'],\n",
    "    ['final', 'example', 'sentence']\n",
    "]\n",
    "\n",
    "# CBOW Model\n",
    "cbow_model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Skip-gram Model\n",
    "skipgram_model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=5, min_count=1, sg=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector for 'example' in CBOW model: [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
      "Similar words to 'example' in CBOW model: [('one', 0.06796975433826447), ('an', 0.033640578389167786), ('final', 0.009391162544488907), ('another', 0.0045030261389911175), ('sentence', -0.010839177295565605), ('more', -0.023677315562963486), ('is', -0.09575345367193222), ('and', -0.11410745233297348), ('yet', -0.11555545777082443), ('this', -0.13429947197437286)]\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding vector for a specific word\n",
    "print(\"Embedding vector for 'example' in CBOW model:\", cbow_model.wv['example'])\n",
    "\n",
    "# Find similar words\n",
    "similar_words = cbow_model.wv.most_similar('example')\n",
    "print(\"Similar words to 'example' in CBOW model:\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x00000136648476A0>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'$ESI on lows, down $1.50 to $2.50 BK a real possibility'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001363AD75150>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001365B123AC0>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'According to the Finnish-Russian Chamber of Commerce , all the major construction companies of Finland are operating in Russia .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001363AD75150>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The Swedish buyout firm has sold its remaining 22.4 percent stake , almost eighteen months after taking the company public in Finland .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001365B123AC0>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"$SPY wouldn't be surprised to see a green close\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001363AD75150>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "for i in range(0,6):\n",
    "    #print(df.Sentence[i])\n",
    "    #text = \"This is an example sentence for preprocessing.\"\n",
    "    cbow_model = Word2Vec(df.Sentence[i], vector_size=100, window=5, min_count=1, sg=0)\n",
    "    display(df.Sentence[i])\n",
    "    display(cbow_model.build_vocab_from_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Sentence Sentiment  \\\n",
      "0  This is an example sentence for preprocessing.  Positive   \n",
      "1         Another example sentence to preprocess.  Negative   \n",
      "2                   Yet another example sentence.  Positive   \n",
      "3                      One more example sentence.  Negative   \n",
      "4                                   And one more.   Neutral   \n",
      "5                         Final example sentence.  Positive   \n",
      "\n",
      "                          Preprocessed_Text  \n",
      "0        [example, sentence, preprocessing]  \n",
      "1  [another, example, sentence, preprocess]  \n",
      "2         [yet, another, example, sentence]  \n",
      "3                  [one, example, sentence]  \n",
      "4                                     [one]  \n",
      "5                [final, example, sentence]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Sentence': [\n",
    "        \"This is an example sentence for preprocessing.\",\n",
    "        \"Another example sentence to preprocess.\",\n",
    "        \"Yet another example sentence.\",\n",
    "        \"One more example sentence.\",\n",
    "        \"And one more.\",\n",
    "        \"Final example sentence.\"\n",
    "    ],\n",
    "    'Sentiment': [\n",
    "        'Positive',\n",
    "        'Negative',\n",
    "        'Positive',\n",
    "        'Negative',\n",
    "        'Neutral',\n",
    "        'Positive'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Create a list to store preprocessed text data\n",
    "preprocessed_texts = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for i in range(len(df)):\n",
    "    preprocessed_text = preprocess_text(df['Sentence'][i])\n",
    "    preprocessed_texts.append(preprocessed_text)\n",
    "\n",
    "# Add preprocessed text to the DataFrame\n",
    "df['Preprocessed_Text'] = preprocessed_texts\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
