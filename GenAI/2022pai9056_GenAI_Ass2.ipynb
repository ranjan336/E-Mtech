{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ranjan.patra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ranjan.patra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (227371831.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [109]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(df.[0])\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = r\"C:\\Users\\ranjan.patra\\OneDrive - Lingaro Sp. z o. o\\DATA\\IITJ\\Course\\3rd Sem\\NLP\\Assignment_2\\data.csv\"  # Replace 'path_to_your_file.csv' with the actual path to your CSV file\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.Sentence[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5842\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>3130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment  count\n",
       "0  negative    860\n",
       "1   neutral   3130\n",
       "2  positive   1852"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(df))\n",
    "\n",
    "display(df.groupby('Sentiment').size().reset_index(name='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SentenceID Sentiment                                  Original Sentence  \\\n",
      "0            S0  positive  The GeoSolutions technology will leverage Bene...   \n",
      "1            S1  negative  $ESI on lows, down $1.50 to $2.50 BK a real po...   \n",
      "2            S2  positive  For the last quarter of 2010 , Componenta 's n...   \n",
      "3            S3   neutral  According to the Finnish-Russian Chamber of Co...   \n",
      "4            S4   neutral  The Swedish buyout firm has sold its remaining...   \n",
      "...         ...       ...                                                ...   \n",
      "5837      S5837  negative  RISING costs have forced packaging producer Hu...   \n",
      "5838      S5838   neutral  Nordic Walking was first used as a summer trai...   \n",
      "5839      S5839   neutral  According shipping company Viking Line , the E...   \n",
      "5840      S5840   neutral  In the building and home improvement trade , s...   \n",
      "5841      S5841  positive  HELSINKI AFX - KCI Konecranes said it has won ...   \n",
      "\n",
      "                                         Processed_text  \n",
      "0     [geosolutions, technology, leverage, benefon, ...  \n",
      "1          [esi, lows, 150, 250, bk, real, possibility]  \n",
      "2     [last, quarter, 2010, componenta, net, sales, ...  \n",
      "3     [according, finnishrussian, chamber, commerce,...  \n",
      "4     [swedish, buyout, firm, sold, remaining, 224, ...  \n",
      "...                                                 ...  \n",
      "5837  [rising, costs, forced, packaging, producer, h...  \n",
      "5838  [nordic, walking, first, used, summer, trainin...  \n",
      "5839  [according, shipping, company, viking, line, e...  \n",
      "5840  [building, home, improvement, trade, sales, de...  \n",
      "5841  [helsinki, afx, kci, konecranes, said, order, ...  \n",
      "\n",
      "[5842 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "preprocessed_data = []\n",
    "for i in range(len(df)):\n",
    "    # print(i)\n",
    "    # print(df.Sentence[i])\n",
    "    #text = \"This is an example sentence for preprocessing.\"\n",
    "    preprocessed_text = preprocess_text(df['Sentence'][i])\n",
    "    preprocessed_data.append({\n",
    "    'SentenceID':\"S\"+str(i),\n",
    "    'Sentiment':df.Sentiment[i],\n",
    "    'Original Sentence':df.Sentence[i],\n",
    "    'Processed_text':preprocessed_text}\n",
    "                              )\n",
    "\n",
    "preprocessed_df = pd.DataFrame(preprocessed_data)\n",
    "print(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [geosolutions, technology, leverage, benefon, ...\n",
      "1            [esi, lows, 150, 250, bk, real, possibility]\n",
      "2       [last, quarter, 2010, componenta, net, sales, ...\n",
      "3       [according, finnishrussian, chamber, commerce,...\n",
      "4       [swedish, buyout, firm, sold, remaining, 224, ...\n",
      "                              ...                        \n",
      "5837    [rising, costs, forced, packaging, producer, h...\n",
      "5838    [nordic, walking, first, used, summer, trainin...\n",
      "5839    [according, shipping, company, viking, line, e...\n",
      "5840    [building, home, improvement, trade, sales, de...\n",
      "5841    [helsinki, afx, kci, konecranes, said, order, ...\n",
      "Name: Processed_text, Length: 5842, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_df.Processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example preprocessed data (list of tokenized sentences)\n",
    "preprocessed_data = [\n",
    "    ['this', 'is', 'an', 'example', 'sentence'],\n",
    "    ['another', 'example', 'sentence'],\n",
    "    ['yet', 'another', 'example', 'sentence'],\n",
    "    ['one', 'more', 'example', 'sentence'],\n",
    "    ['and', 'one', 'more'],\n",
    "    ['final', 'example', 'sentence']\n",
    "]\n",
    "\n",
    "# CBOW Model\n",
    "cbow_model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=1, min_count=1, sg=0)\n",
    "\n",
    "# Skip-gram Model\n",
    "skipgram_model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=1, min_count=1, sg=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector for 'improvement' in CBOW model: [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
      "Similar words to 'improvement' in CBOW model: [('one', 0.06797593832015991), ('an', 0.033640578389167786), ('final', 0.009391162544488907), ('another', 0.0045030261389911175), ('sentence', -0.010839177295565605), ('more', -0.023671656847000122), ('is', -0.09575345367193222), ('and', -0.11410737037658691), ('yet', -0.11555545777082443), ('this', -0.13429947197437286)]\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding vector for a specific word\n",
    "print(\"Embedding vector for 'improvement' in CBOW model:\", cbow_model.wv['example'])\n",
    "\n",
    "# Find similar words\n",
    "similar_words = cbow_model.wv.most_similar('example')\n",
    "print(\"Similar words to 'improvement' in CBOW model:\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.11)\n",
      "Requirement already satisfied: networkx in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ranjan.patra\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [119]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     73\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m context_words, target_word \u001b[38;5;129;01min\u001b[39;00m cbow_loader:\n\u001b[0;32m     75\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     76\u001b[0m         output \u001b[38;5;241m=\u001b[39m cbow_model(context_words)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [119]\u001b[0m, in \u001b[0;36mCBOWDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences):\n\u001b[1;32m---> 41\u001b[0m         context_words\u001b[38;5;241m.\u001b[39mappend(\u001b[43mword2idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(context_words), torch\u001b[38;5;241m.\u001b[39mtensor(word2idx[target_word])\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example preprocessed data (list of tokenized sentences)\n",
    "preprocessed_data = [\n",
    "    ['this', 'is', 'an', 'example', 'sentence'],\n",
    "    ['another', 'example', 'sentence'],\n",
    "    ['yet', 'another', 'example', 'sentence'],\n",
    "    ['one', 'more', 'example', 'sentence'],\n",
    "    ['and', 'one', 'more'],\n",
    "    ['final', 'example', 'sentence']\n",
    "]\n",
    "\n",
    "# Create vocabulary\n",
    "word_counts = Counter()\n",
    "for sentence in preprocessed_data:\n",
    "    word_counts.update(sentence)\n",
    "\n",
    "word2idx = {word: idx for idx, (word, _) in enumerate(word_counts.items())}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# Define CBOW Dataset\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, sentences, window_size):\n",
    "        self.sentences = sentences\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_word = self.sentences[idx][self.window_size]\n",
    "        context_words = []\n",
    "        for i in range(-self.window_size, self.window_size + 1):\n",
    "            if i != 0 and idx + i >= 0 and idx + i < len(self.sentences):\n",
    "                context_words.append(word2idx[self.sentences[idx + i]])\n",
    "        return torch.tensor(context_words), torch.tensor(word2idx[target_word])\n",
    "\n",
    "# Define CBOW Model\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs).sum(dim=1)\n",
    "        output = self.linear(embedded)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 1\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Create CBOW Dataset and DataLoader\n",
    "cbow_dataset = CBOWDataset(preprocessed_data, window_size)\n",
    "cbow_loader = DataLoader(cbow_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Initialize CBOW model, loss function, and optimizer\n",
    "cbow_model = CBOW(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for CBOW model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for context_words, target_word in cbow_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = cbow_model(context_words)\n",
    "        loss = criterion(output, target_word)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss}')\n",
    "\n",
    "# Define Skip-gram Dataset\n",
    "class SkipgramDataset(Dataset):\n",
    "    def __init__(self, sentences, window_size):\n",
    "        self.sentences = sentences\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_word = self.sentences[idx][self.window_size]\n",
    "        context_words = []\n",
    "        for i in range(-self.window_size, self.window_size + 1):\n",
    "            if i != 0 and idx + i >= 0 and idx + i < len(self.sentences):\n",
    "                context_words.append(word2idx[self.sentences[idx + i]])\n",
    "        return torch.tensor(word2idx[target_word]), torch.tensor(context_words)\n",
    "\n",
    "# Define Skip-gram Model\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_word):\n",
    "        embedded = self.embedding(target_word)\n",
    "        output = self.linear(embedded)\n",
    "        return output\n",
    "\n",
    "# Create Skip-gram Dataset and DataLoader\n",
    "skipgram_dataset = SkipgramDataset(preprocessed_data, window_size)\n",
    "skipgram_loader = DataLoader(skipgram_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Initialize Skip-gram model, loss function, and optimizer\n",
    "skipgram_model = Skipgram(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(skipgram_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for Skip-gram model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for target_word, context_words in skipgram_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = skipgram_model(target_word)\n",
    "        loss = criterion(output, context_words)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x00000136648476A0>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'$ESI on lows, down $1.50 to $2.50 BK a real possibility'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001363AD75150>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001365B123AC0>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'According to the Finnish-Russian Chamber of Commerce , all the major construction companies of Finland are operating in Russia .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001363AD75150>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The Swedish buyout firm has sold its remaining 22.4 percent stake , almost eighteen months after taking the company public in Finland .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001365B123AC0>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"$SPY wouldn't be surprised to see a green close\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.build_vocab_from_freq of <gensim.models.word2vec.Word2Vec object at 0x000001363AD75150>>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "for i in range(0,6):\n",
    "    #print(df.Sentence[i])\n",
    "    #text = \"This is an example sentence for preprocessing.\"\n",
    "    cbow_model = Word2Vec(df.Sentence[i], vector_size=100, window=5, min_count=1, sg=0)\n",
    "    display(df.Sentence[i])\n",
    "    display(cbow_model.build_vocab_from_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Sentence Sentiment  \\\n",
      "0  This is an example sentence for preprocessing.  Positive   \n",
      "1         Another example sentence to preprocess.  Negative   \n",
      "2                   Yet another example sentence.  Positive   \n",
      "3                      One more example sentence.  Negative   \n",
      "4                                   And one more.   Neutral   \n",
      "5                         Final example sentence.  Positive   \n",
      "\n",
      "                          Preprocessed_Text  \n",
      "0        [example, sentence, preprocessing]  \n",
      "1  [another, example, sentence, preprocess]  \n",
      "2         [yet, another, example, sentence]  \n",
      "3                  [one, example, sentence]  \n",
      "4                                     [one]  \n",
      "5                [final, example, sentence]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Sentence': [\n",
    "        \"This is an example sentence for preprocessing.\",\n",
    "        \"Another example sentence to preprocess.\",\n",
    "        \"Yet another example sentence.\",\n",
    "        \"One more example sentence.\",\n",
    "        \"And one more.\",\n",
    "        \"Final example sentence.\"\n",
    "    ],\n",
    "    'Sentiment': [\n",
    "        'Positive',\n",
    "        'Negative',\n",
    "        'Positive',\n",
    "        'Negative',\n",
    "        'Neutral',\n",
    "        'Positive'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Create a list to store preprocessed text data\n",
    "preprocessed_texts = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for i in range(len(df)):\n",
    "    preprocessed_text = preprocess_text(df['Sentence'][i])\n",
    "    preprocessed_texts.append(preprocessed_text)\n",
    "\n",
    "# Add preprocessed text to the DataFrame\n",
    "df['Preprocessed_Text'] = preprocessed_texts\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
